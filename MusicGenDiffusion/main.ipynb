{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e5430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from copy import copy\n",
    "from torch.utils.data import random_split\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def show_images(data, num_samples=1, cols=1):\n",
    "    if isinstance(data, torchvision.datasets.ImageFolder):\n",
    "        plt.figure(figsize=(15,15))\n",
    "        for i, img in enumerate(data):\n",
    "            if i == num_samples:\n",
    "                break\n",
    "            plt.subplot(num_samples/cols+1, cols, i+1)\n",
    "            plt.imshow(img[0], color='red')\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0]\n",
    "    image = torch.clamp(image, -1, 1)\n",
    "    plt.imshow(image.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3cb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 150\n",
    "max_octaves = 8\n",
    "sequence_length = 64\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "#####################\n",
    "def linear_beta_schedule(timesteps, start = 0.0001, end = 0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def cosine_beta_schedule(timesteps, start = 0.0001, end = 0.02):\n",
    "    t = torch.linspace(0, 1, timesteps)\n",
    "    return start + (end - start) * (1 - (1 + torch.cos(np.pi * t)) / 2)\n",
    "\n",
    "betas = cosine_beta_schedule(timesteps=T).cuda()\n",
    "\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0).cuda()\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0).cuda()\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas).cuda()\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).cuda()\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod).cuda()\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod).cuda()    \n",
    "\n",
    "def deformat_time(input):\n",
    "        input = torch.tensor(input) \n",
    "        return torch.clamp(input*2000, 0)\n",
    "def format_time(input):\n",
    "        return input/2000\n",
    "    \n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cuda())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t, device =\"cuda\"):\n",
    "    noise = torch.randn_like(x_0)\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "    \n",
    "    return sqrt_alphas_cumprod_t.to(device)*x_0.to(device) + \\\n",
    "sqrt_one_minus_alphas_cumprod_t.to(device)*noise.to(device), noise.to(device) \n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, in_shape, emb_shape):\n",
    "        super().__init__()\n",
    "        self.sems = 12\n",
    "        self.octs = max_octaves\n",
    "        self.sem_shape = emb_shape - 1\n",
    "        self.emb_shape = emb_shape\n",
    "    \n",
    "        self.sem_embedding = nn.Sequential(nn.Linear(in_shape-1, self.sem_shape), nn.SiLU(),\n",
    "                                        nn.Linear(self.sem_shape, self.sem_shape), nn.Tanh())\n",
    "        \n",
    "        self.sem_decoder = nn.Sequential(nn.Linear(self.sem_shape, self.sem_shape), nn.SiLU(),\n",
    "                                        nn.Linear(self.sem_shape, in_shape-1), nn.SiLU())\n",
    "        \n",
    "    def encoder(self, input):\n",
    "        encoded_sem = self.sem_embedding(input[..., :-1])\n",
    "        return torch.cat((encoded_sem, input[..., -1:]), dim=-1)\n",
    "    \n",
    "    def decoder(self, input):\n",
    "        decoded_sem = self.sem_decoder(input[..., :-1]).to(device)\n",
    "        return torch.cat((decoded_sem, input[..., -1:]), dim=-1)\n",
    "        \n",
    "    def fit(self, target_loss):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=0.00025)\n",
    "        N = 256\n",
    "        while True:\n",
    "            one_hot1 = torch.eye(self.sems)\n",
    "            one_hot2 = torch.eye(self.octs)\n",
    "            random_values = torch.rand(N, 1)\n",
    "            random_values[N//2:] *= 0.3\n",
    "            random_values = format_time(random_values*2500)\n",
    "            random_values[:N//4] = 0\n",
    "            input = torch.cat([one_hot1[torch.randint(0, self.sems, (N,))] , one_hot2[torch.randint(0, self.octs, (N,))], random_values], dim=1)\n",
    "            input = input.unsqueeze(0).to(device)\n",
    "            out = self.decoder(self.encoder(input))\n",
    "            loss = nn.functional.mse_loss(input, out)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            print(loss.item())\n",
    "            if loss.item() < target_loss:\n",
    "                torch.save(self.state_dict(), \"./modeldata/embedding\")\n",
    "                break\n",
    "                \n",
    "embedding = Embedding(13+max_octaves, 12).to(device)\n",
    "#embedding.fit(0.00005)\n",
    "emb_state_dict = torch.load(\"./modeldata/embedding\")\n",
    "embedding.load_state_dict(emb_state_dict)\n",
    "embedding.to(device)\n",
    "\n",
    "for param in embedding.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_count = [0,0,0]   \n",
    "\n",
    "class NoteDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, sequence_length = 128):\n",
    "        training_data = pickle.load(open(data_path, 'rb'))\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        for n, track in enumerate(training_data):\n",
    "            i=0\n",
    "            while i < len(track[1])-1:\n",
    "                time_diff = track[1][i+1][\"start\"] - track[1][i][\"start\"]\n",
    "                rev_time_diff = track[1][i][\"start\"] - track[1][i-1][\"start\"]\n",
    "                notes_the_same = track[1][i][\"notes\"] == track[1][i+1][\"notes\"]\n",
    "                rev_notes_the_same = track[1][i][\"notes\"] == track[1][i-1][\"notes\"]\n",
    "                if (time_diff < 50. and notes_the_same) or (rev_time_diff < 50. and rev_notes_the_same) or (int(time_diff) in range(1,65) or time_diff < 0.):\n",
    "\n",
    "                    training_data[n][1].remove(training_data[n][1][i])\n",
    "                    skip_count[0] += 1\n",
    "                else:\n",
    "                    i+=1\n",
    "        \n",
    "        for track in training_data:\n",
    "            for i in reversed(range(len(track[1]))):\n",
    "                time_diff = track[1][i][\"start\"] - track[1][i-1][\"start\"]\n",
    "                track[1][i][\"start\"] = 0 if (i==0 or -1 < time_diff < 0) else int(time_diff)\n",
    "           \n",
    "        self.data = []\n",
    "        self.band_idx = []#name i stores data i name\n",
    "        self.bands = []#band list\n",
    "        \n",
    "        def extract_data(start_index = 0):\n",
    "            for track in training_data:\n",
    "                sequence_ind = 0\n",
    "                sequence_data = []\n",
    "                track[1] = track[1][start_index:]\n",
    "                for beat in track[1]:\n",
    "                    if  beat[\"start\"] < 0:\n",
    "                        skip_count[1] += 1\n",
    "                        continue\n",
    "                    new_elements = beat[\"notes\"]\n",
    "                    for note in new_elements:\n",
    "                        if note[1] > (max_octaves-1):\n",
    "                            skip_count[2] += 1\n",
    "                            continue\n",
    "                        new_data_point = [[0 for i in range(12)], [0 for i in range(max_octaves)]]\n",
    "                        new_data_point[0][note[0]], new_data_point[1][note[1]] = 1, 1\n",
    "                        new_data_point = new_data_point[0] + new_data_point[1]\n",
    "                        new_data_point.append(min(beat[\"start\"], 2500))\n",
    "                        sequence_data.append(new_data_point) \n",
    "                        sequence_ind+=1\n",
    "                        if sequence_ind == sequence_length:\n",
    "                            self.data.append(sequence_data)\n",
    "                            if track[0] not in self.bands:\n",
    "                                self.bands.append(track[0])\n",
    "                            self.band_idx.append(self.bands.index(track[0]))\n",
    "                            sequence_data = []\n",
    "                            sequence_ind = 0\n",
    "                            \n",
    "        extract_data()\n",
    "        extract_data(sequence_length//2)\n",
    "        \n",
    "        self.data = torch.tensor(self.data, dtype=torch.float32).to(device)\n",
    "        pickle.dump(self.bands, open(\"./modeldata/bands\", 'wb'))\n",
    "        \n",
    "        print(\"skipped:\", skip_count, \"total_bands:\", len(self.bands))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        rnd = (torch.rand(1).to(device)*-0.25) + 1.\n",
    "        data = self.data[idx].clone()\n",
    "        data[:, -1] = format_time(data[:,-1]*rnd)\n",
    "        return data, self.bands[self.band_idx[idx]]\n",
    "    \n",
    "class SelfAttention(nn.Module): #attends token to token\n",
    "    def __init__(self, channels, size1, size2, num_heads = 4, emb_dim=32):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size1 = size1\n",
    "        self.size2 = size2\n",
    "        self.hidden_dim = channels*size2\n",
    "        self.mha = nn.MultiheadAttention(self.hidden_dim, num_heads, batch_first=True)\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([self.hidden_dim]),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.emb_layer = nn.Sequential(nn.SiLU(), nn.Linear(emb_dim, channels))\n",
    "\n",
    "    def forward(self, x, emb=0):\n",
    "        if type(emb) != int:\n",
    "            emb = self.emb_layer(emb)[(..., ) + (None, ) * 2]\n",
    "            \n",
    "        x = (x+emb).swapaxes(1, 2).reshape(-1, self.size1, self.hidden_dim)\n",
    "        attention_value, _ = self.mha(x, x, x)\n",
    "        \n",
    "\n",
    "        \n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value \n",
    "        return attention_value.reshape(-1, self.size1, self.channels, self.size2).swapaxes(1, 2)\n",
    "    \n",
    "\"\"\"\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size1, size2, num_heads = 4):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size1 = size1\n",
    "        self.size2 = size2\n",
    "        self.mha = nn.MultiheadAttention(channels, num_heads, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size1 * self.size2).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size1, self.size2)\n",
    "    \n",
    "\"\"\"    \n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False, pad=1, stride = 1):\n",
    "        super().__init__()\n",
    "        if mid_channels == None:\n",
    "            mid_channels = out_channels\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.double_conv = nn.Sequential(nn.Conv2d(in_channels, mid_channels, 3, padding = pad, stride=stride, bias = False),\n",
    "                                         nn.GroupNorm(1, mid_channels),\n",
    "                                         nn.GELU(),\n",
    "                                         nn.Conv2d(mid_channels, out_channels, 3, padding = pad, stride=stride, bias = False),\n",
    "                                         nn.GroupNorm(1, out_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(self.double_conv(x)+x)\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "        \n",
    "class ConditionalDoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False, pad=1, stride = 1, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, out_channels, mid_channels, residual, pad, stride)\n",
    "        \n",
    "        self.emb_layer = nn.Sequential(nn.SiLU(),\n",
    "                                       nn.Linear(emb_dim, out_channels))\n",
    "        \n",
    "    def forward(self, x, cond):\n",
    "        x = self.double_conv(x)\n",
    "        emb = self.emb_layer(cond)[(..., ) + (None, ) * 2]\n",
    "        return x + emb\n",
    "        \n",
    "        \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=32, pad=1):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual = True, pad=pad),\n",
    "            DoubleConv(in_channels, out_channels, pad=pad)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.emb_layer = nn.Sequential(nn.SiLU(),\n",
    "                                       nn.Linear(emb_dim, out_channels))\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[(..., ) + (None, ) * 2]\n",
    "        return x + emb\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor = 2, mode=\"bilinear\", align_corners = True)\n",
    "        self.conv = nn.Sequential(DoubleConv(in_channels, in_channels, residual = True),\n",
    "                                 DoubleConv(in_channels, out_channels, in_channels // 2))\n",
    "        \n",
    "        self.emb_layer = nn.Sequential(nn.SiLU(),\n",
    "                                       nn.Linear(emb_dim, out_channels))\n",
    "    def forward(self, x, skip_x, t):\n",
    "                                \n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, skip_x], dim = 1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[(..., ) + (None, ) * 2]\n",
    "        return x + emb\n",
    "        \n",
    "class TimeAndBandEmbedding(nn.Module):\n",
    "    def __init__(self, dim_t, dim_b):\n",
    "        super().__init__()\n",
    "        self.dim_t = dim_t\n",
    "        self.dim_b = dim_b\n",
    "        self.bands_list = pickle.load(open(\"./modeldata/bands\", 'rb'))\n",
    "        self.band_compressor = nn.Sequential(nn.Linear(len(self.bands_list), self.dim_b), \n",
    "                                             nn.SiLU())\n",
    "\n",
    "    def forward(self, time, bands):\n",
    "        band_id = F.one_hot(torch.tensor([self.bands_list.index(b) for b in bands]).to(\"cuda\"), len(self.bands_list)).to(torch.float)\n",
    "        half_dim_t = self.dim_t // 2\n",
    "        embeddings_t = math.log(10000) / (half_dim_t - 1)\n",
    "        embeddings_t = torch.exp(torch.arange(half_dim_t, device=device) * -embeddings_t)\n",
    "        embeddings_t = time[:, None] * embeddings_t[None, :]\n",
    "        embeddings_t = torch.cat((embeddings_t.sin(), embeddings_t.cos()), dim=-1)\n",
    "        embeddings_b = self.band_compressor(band_id)\n",
    "        return torch.cat((embeddings_t, embeddings_b), dim=-1)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, time_dim=26, band_dim = 6):\n",
    "        super().__init__()\n",
    "        self.encoding = TimeAndBandEmbedding(time_dim, band_dim)\n",
    "        self.b_d = band_dim\n",
    "        seq = sequence_length\n",
    "        embed_dim = time_dim + band_dim\n",
    "\n",
    "        self.inc = DoubleConv(c_in, 32)     \n",
    "        self.down1 = Down(32, 64, embed_dim) \n",
    "        self.sa1 = SelfAttention(64, seq//2, 6, 4,embed_dim)\n",
    "        self.down2 = Down(64, 128, embed_dim)  \n",
    "        self.sa2 = SelfAttention(128, seq//4, 3, 4,embed_dim) \n",
    " \n",
    "        self.bot1 = ConditionalDoubleConv(128, 128, emb_dim=embed_dim)\n",
    "        self.bottleneck_attention = [SelfAttention(128, seq//4, 3, 8 ,embed_dim).to(device) for i in range(4)]\n",
    "        self.bot2 = ConditionalDoubleConv(128, 128, emb_dim=embed_dim) \n",
    "        \n",
    "        self.up1 = Up(192, 128, embed_dim) \n",
    "        self.sa3 = SelfAttention(128, seq//2, 6, 4,embed_dim)\n",
    "        self.up2 = Up(160, 64, embed_dim)\n",
    "        self.sa4 = SelfAttention(64, seq, 12, 4, embed_dim)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "        \n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                \n",
    "    def sab(self, x, cond):\n",
    "        for a in self.bottleneck_attention:\n",
    "            x = a(x, cond)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, t, b):\n",
    "        \n",
    "        cond = self.encoding(t, b)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, cond)\n",
    "        x2 = self.sa1(x2,cond)\n",
    "        x3 = self.down2(x2, cond)\n",
    "        x3 = self.sa2(x3,cond)\n",
    "        \n",
    "        x3 = self.bot1(x3, cond)\n",
    "        x3 = self.sab(x3,cond)\n",
    "        x3 = self.bot2(x3, cond)\n",
    "        \n",
    "        x = self.up1(x3, x2, cond)\n",
    "        x = self.sa3(x,cond)     \n",
    "        x = self.up2(x, x1, cond)\n",
    "        x = self.sa4(x,cond)\n",
    "        output = self.outc(x)\n",
    "        return output      \n",
    "    \n",
    "def get_loss_comps(model, x_0, t, bands):\n",
    "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "    noise_pred = model(x_noisy, t, bands)\n",
    "    return noise, noise_pred\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x,t,b,temp):\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "   \n",
    "    model_mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, b) / sqrt_one_minus_alphas_cumprod_t)\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise * temp\n",
    "    \n",
    "@torch.no_grad()\n",
    "def generate(b, start_temp, end_temp, img=None, context=0):\n",
    "    model.eval()\n",
    "    if img is None:\n",
    "        img = torch.randn((1, 1, sequence_length, embedding.emb_shape), device=device)\n",
    "\n",
    "        \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 5\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        temp = start_temp + (end_temp-start_temp)*(i/T)\n",
    "        img[:,:,context:] = sample_timestep(img, t, [b], temp)[:,:,context:] \n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i/stepsize+1))\n",
    "            show_tensor_image(img.detach().cpu())\n",
    "    plt.show()\n",
    "    model.train()\n",
    "    return img\n",
    "    \n",
    "\n",
    "def generate_sequence(b, start_temp, end_temp, number, context):\n",
    "    seq = generate(b, start_temp, end_temp)\n",
    "    for i in range(number-1):\n",
    "        new = seq.clone()[:, :,-context:]\n",
    "        new = torch.cat((new, torch.randn((1, 1, sequence_length-context, embedding.emb_shape), device=device)), dim = 2)\n",
    "        new = generate(b, start_temp, end_temp, new, context)[:,:,context:]\n",
    "        seq = torch.cat((seq,new),dim=2)\n",
    "    return seq\n",
    "        \n",
    "    \n",
    "def prediction(input, k=1):\n",
    "    input = input.squeeze(0).squeeze(0)\n",
    "    sems = torch.tensor(torch.topk(input[:, :12], dim=-1, k=k).indices[:, k-1]).unsqueeze(1)\n",
    "    octs =  torch.tensor(torch.topk(input[:, 12:-1], dim=-1, k=k).indices[:, k-1]).unsqueeze(1)\n",
    "    times = input[:, -1].unsqueeze(-1)\n",
    "    return torch.cat((sems,octs,deformat_time(times)), dim = -1).to(torch.int).to(\"cpu\").detach().numpy()\n",
    "\n",
    "def prediction2(input):\n",
    "    input = input.squeeze(0).squeeze(0)\n",
    "    sems = torch.tensor(input[:, :12].argmax(dim=-1)).unsqueeze(1)\n",
    "    octs =  torch.tensor(input[:,12:12+max_octaves].argmax(dim=-1)).unsqueeze(1)\n",
    "    print(octs.shape)\n",
    "    times = input[:, -1].unsqueeze(-1)\n",
    "    return torch.cat((sems,octs,deformat_time(times)), dim = -1).to(torch.int).to(\"cpu\").detach().numpy()\n",
    "\n",
    "def group_times(input, num_groups, coeff, clamp, make_time_constant):\n",
    "    if make_time_constant[\"value\"] != -1:\n",
    "        out = input.copy()\n",
    "        out[:, -1] = make_time_constant[\"value\"]\n",
    "        mask = input[:, -1] < make_time_constant[\"threshold\"]\n",
    "        out[mask, -1] = 0\n",
    "        return out\n",
    "    times = input[:, -1].flatten()\n",
    "    times[times>clamp] = clamp\n",
    "    value_range = max(times) - min(times)\n",
    "    bin_size = value_range / num_groups\n",
    "    groups = [int((time - min(times)) //bin_size )for time in times]\n",
    "    \n",
    "    mean_per_group = [[] for i in range(num_groups+1)]\n",
    "    for g, t in enumerate(times):\n",
    "        mean_per_group[int(groups[g])].append(t)\n",
    "    \n",
    "    mean_per_group = [sum(e)/len(e) if len(e)!= 0 else 0 for e in mean_per_group] \n",
    "    corrected_ts = [mean_per_group[i] for i in groups]\n",
    "    out = [np.append(elem[:-1],int(corrected_ts[i]*coeff)) for i, elem in enumerate(input)]\n",
    "    return out\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1eb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def lr_lambda(epoch, max_epochs, max_lr, initial_lr, final_lr, warmup_epochs):\n",
    "    \n",
    "    if epoch < warmup_epochs:\n",
    "        return (max_lr - initial_lr) / warmup_epochs * epoch + initial_lr\n",
    "    else:\n",
    "        return (final_lr + 0.5 * (max_lr - final_lr) *\n",
    "                (1 + np.cos((epoch - warmup_epochs) / (max_epochs - warmup_epochs) * np.pi)))\n",
    "\n",
    "max_epochs = 500\n",
    "max_lr = 10**(-3.8)\n",
    "initial_lr = 10**(-6)\n",
    "final_lr = 10**(-8)\n",
    "warmup_epochs = 5\n",
    "\"\"\"\n",
    "dataset = NoteDataset(data_path = \"./preprocessed_data/data\", sequence_length = sequence_length)\n",
    "train_size = int(0.95 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\"\"\"\n",
    "\n",
    "model = UNet(c_in=1, c_out=1, time_dim=30, band_dim = 2).to(device)\n",
    "net_state_dict = torch.load(\"./modeldata/model\")\n",
    "model.load_state_dict(net_state_dict) \n",
    "optim = torch.optim.Adam(model.parameters(), lr=1, weight_decay=0.0005)\n",
    "scheduler = LambdaLR(optim, lr_lambda=lambda epoch: lr_lambda(epoch, max_epochs, max_lr, initial_lr, final_lr, warmup_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f89a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scheduler.step()\n",
    "#optim = torch.optim.Adam(model.parameters(), lr=10**(-7.5), weight_decay=0.0005)\n",
    "epochs = 15000\n",
    "\n",
    "def val():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for images, bands in val_loader:\n",
    "        images = images.unsqueeze(1)\n",
    "        images = embedding.encoder(images)\n",
    "        t = torch.randint(0, T, (images.shape[0],), device=device).long()\n",
    "        target, prediction = get_loss_comps(model, images, t, bands)\n",
    "        loss = F.mse_loss(prediction, target)\n",
    "        losses.append(loss.item())\n",
    "    model.train()\n",
    "    loss = sum(losses)/len(losses)\n",
    "    print(f\"Validation loss: {loss} \")\n",
    "    return loss\n",
    "            \n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        val_losses = [99,99,99]\n",
    "        for images, bands in data_loader:\n",
    "            images = images.unsqueeze(1)\n",
    "            images = embedding.encoder(images)\n",
    "            optim.zero_grad()\n",
    "            t = torch.randint(0, T, (images.shape[0],), device=device).long()\n",
    "            target, prediction = get_loss_comps(model, images, t, bands)\n",
    "            loss = F.mse_loss(prediction, target)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        scheduler.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch} | Loss: {sum(losses)/len(losses)} \")\n",
    "        if epoch % 150 == 0 and epoch != 0:      \n",
    "            torch.save(model.state_dict(), \"./modeldata/modelv2\")\n",
    "        if epoch % 2 == 0 and epoch != 0:      \n",
    "            val_losses.append(val())\n",
    "            if val_losses[-1] > val_losses[-2] > val_losses[-3]:\n",
    "                torch.save(model.state_dict(), \"./modeldata/modelv2\")\n",
    "                print(\"Validation loss increasing\")\n",
    "                break\n",
    "            \n",
    "train()   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ae497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = generate_sequence(\"tech\", start_temp=1, end_temp=1, number=3, context=30) #context < sequence_length\n",
    "pred = embedding.decoder(pred)\n",
    "pred = prediction2(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1465478c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = group_times(pred, num_groups=1, coeff=1, clamp=600, \n",
    "                  make_time_constant = {\"value\": 120, \"threshold\": 0}) #if make_time_constant[\"value\"]!=-1 replaces all times, except those below threshold, which are set to 0\n",
    "#out = pred                                                                                                    \n",
    "with open(\"./outputs/generated_sequence\", \"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31363515",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./modeldata/model\")"
   ]
  },

  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = prediction2(dataset.data[13525])\n",
    "for i in range(len(out)):\n",
    "    out[i][2] = format_time(out[i][2])\n",
    "with open(\"./outputs/generated_sequence\", \"wb\") as f:\n",
    "    pickle.dump(out, f)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5994772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.cpu()\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "mainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
